"""
Weekly MOT + Rain + Day Type Data Collection DAG with Quality Checks v2.0

This DAG orchestrates a comprehensive data collection pipeline with quality validation:
- MOT transportation data collection with anomaly detection
- Rain weather data enrichment from 40+ Bangkok stations
- Day type classification (workday/holiday/festival)
- Multi-stage quality checks with intelligent branching logic
- XCom data sharing for comprehensive reporting

Author: Data Engineering Team
Created: October 2025
Updated: October 2025 (v2.0 - Fixed trigger rules and branching)
Schedule: Manual trigger for testing (set to "0 2 * * 0" for production)

Features:
- BranchPythonOperator for conditional workflow (FIXED trigger rules)
- XCom for sharing quality metrics between tasks
- Comprehensive data validation at each stage
- Anomaly detection with realistic thresholds (BTS up to 2M passengers)
- Missing data alerts and zero-value detection
- Automatic retry with improved error handling

Improvements in v2.0:
- Fixed trigger_rule issues with BranchPythonOperator
- Added none_failed_min_one_success for downstream tasks
- Improved DAG ID to avoid legacy DAG run conflicts
- Enhanced error handling and retry logic

Usage:
1. Enable the DAG in Airflow UI (new DAG ID: weekly-mot-rain-data-collecting-v2)
2. Trigger manually - should work from first run
3. Monitor logs for data collection and quality reports  
4. Check PostgreSQL features table for validated records
5. View XCom in Airflow UI for quality reports
"""

import os
import psycopg2
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator, BranchPythonOperator
from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
from datetime import datetime, timedelta

###############################################
# Parameters
###############################################
spark_conn = os.environ.get("spark_conn", "spark_conn")
spark_master = "spark://spark:7077"
postgres_driver_jar = "/usr/local/spark/assets/jars/postgresql-42.2.6.jar"

# Database connection parameters
postgres_db = "jdbc:postgresql://postgres:5432/airflow"
postgres_user = "airflow"
postgres_pwd = "airflow"

# Database connection config for Python tasks
DB_CONFIG = {
    "host": "postgres",
    "port": 5432,
    "dbname": "airflow",
    "user": "airflow",
    "password": "airflow"
}

###############################################
# DAG Definition
###############################################
now = datetime.now()

default_args = {
    "owner": "data_engineering_team",
    "depends_on_past": False,
    "start_date": datetime(2025, 10, 1),  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
    "email": ["data-team@company.com"],
    "email_on_failure": False,  # ‡∏õ‡∏¥‡∏î‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    "email_on_retry": False,
    "retries": 2,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° retries ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö dependency issues
    "retry_delay": timedelta(minutes=1),  # ‡∏•‡∏î retry delay
    "max_active_runs": 1
}

dag = DAG(
    dag_id="weekly-mot-rain-data-collecting-v2",  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô DAG ID ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á DAG runs ‡πÄ‡∏Å‡πà‡∏≤
    description="Weekly MOT + Rain + Day Type Data Collection DAG with Quality Checks v2.0",
    default_args=default_args,
    schedule_interval="0 2 * * 0",  # ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå‡πÄ‡∏ß‡∏•‡∏≤ 02:00 ‡∏ô. (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö)
    catchup=False,
    max_active_runs=1,
    tags=["mot", "rain", "day-type", "quality-checks", "branching", "xcom", "v2", "production"]
)

###############################################
# Quality Check Functions
###############################################

def check_mot_data_quality(**context):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MOT ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Collection
    - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏™‡∏≤‡∏¢
    - ‡∏´‡∏≤ Anomalies ‡πÅ‡∏•‡∏∞ Missing data
    - ‡∏™‡πà‡∏á‡∏ú‡∏• XCom ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tasks ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    
    Realistic Transport Data Ranges:
    - BTS: 800,000 - 2,000,000 ‡∏ú‡∏π‡πâ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏£/‡∏ß‡∏±‡∏ô (‡∏™‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡∏Å‡∏ó‡∏°.)
    - MRT Blue: 600,000 - 1,500,000 ‡∏ú‡∏π‡πâ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏£/‡∏ß‡∏±‡∏ô
    - MRT Purple: 200,000 - 800,000 ‡∏ú‡∏π‡πâ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏£/‡∏ß‡∏±‡∏ô  
    - ARL: 50,000 - 200,000 ‡∏ú‡∏π‡πâ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏£/‡∏ß‡∏±‡∏ô
    - SRT Red: 100,000 - 300,000 ‡∏ú‡∏π‡πâ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏£/‡∏ß‡∏±‡∏ô
    - MRT Yellow/Pink: 100,000 - 500,000 ‡∏ú‡∏π‡πâ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏£/‡∏ß‡∏±‡∏ô (‡∏™‡∏≤‡∏¢‡πÉ‡∏´‡∏°‡πà)
    """
    import json
    
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î 7 ‡∏ß‡∏±‡∏ô
    quality_check = {
        "task": "mot_quality_check",
        "timestamp": datetime.now().isoformat(),
        "records_processed": 0,
        "missing_data": {},
        "anomalies": [],
        "quality_score": 0.0,
        "status": "PASS"
    }
    
    try:
        # 1. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô records ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï
        cur.execute("""
            SELECT COUNT(*) FROM features 
            WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
        """)
        quality_check["records_processed"] = cur.fetchone()[0]
        
        # 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing data ‡πÅ‡∏•‡∏∞ Zero values ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡∏≤‡∏¢
        transport_columns = ['arl', 'bts', 'mrt_blue', 'mrt_purple', 'mrt_pink', 'srt_red', 'mrt_yellow']
        
        for column in transport_columns:
            # Missing ‡∏´‡∏£‡∏∑‡∏≠ NULL values
            cur.execute(f"""
                SELECT feature_date FROM features 
                WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
                AND {column} IS NULL
                ORDER BY feature_date
            """)
            null_dates = [str(row[0]) for row in cur.fetchall()]
            
            # Zero values (‡∏ô‡πà‡∏≤‡∏™‡∏á‡∏™‡∏±‡∏¢ - ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ç‡∏ô‡∏™‡πà‡∏á‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏£ 0)
            cur.execute(f"""
                SELECT feature_date FROM features 
                WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
                AND {column} = 0
                ORDER BY feature_date
            """)
            zero_dates = [str(row[0]) for row in cur.fetchall()]
            
            if null_dates or zero_dates:
                issues = []
                if null_dates:
                    issues.extend([f"{date} (NULL)" for date in null_dates])
                if zero_dates:
                    issues.extend([f"{date} (ZERO)" for date in zero_dates])
                quality_check["missing_data"][column] = issues
        
        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Anomalies (‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥) - ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° realistic values
        anomaly_thresholds = {
            'bts': 2000000,        # BTS ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏ú‡∏π‡πâ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏£ 1-2 ‡∏•‡πâ‡∏≤‡∏ô/‡∏ß‡∏±‡∏ô (‡∏õ‡∏Å‡∏ï‡∏¥)
            'mrt_blue': 1500000,   # MRT ‡∏™‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏Ñ‡∏∂‡∏Å‡∏Ñ‡∏±‡∏Å
            'mrt_purple': 800000,  # MRT ‡∏™‡∏≤‡∏¢‡∏°‡πà‡∏ß‡∏á‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
            'arl': 200000,         # ARL ‡πÑ‡∏°‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡∏°‡∏≤‡∏Å
            'srt_red': 300000,     # SRT ‡∏™‡∏≤‡∏¢‡πÅ‡∏î‡∏á‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
            'mrt_yellow': 500000,  # MRT ‡∏™‡∏≤‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡πÜ
            'mrt_pink': 400000     # MRT ‡∏™‡∏≤‡∏¢‡∏ä‡∏°‡∏û‡∏π‡πÉ‡∏´‡∏°‡πà‡πÜ
        }
        
        for column in transport_columns:
            threshold = anomaly_thresholds.get(column, 1000000)  # default 1M
            cur.execute(f"""
                SELECT feature_date, {column} FROM features 
                WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
                AND ({column} > {threshold} OR {column} < 0)  -- ‡πÄ‡∏Å‡∏¥‡∏ô threshold ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏î‡∏•‡∏ö
            """)
            anomalies = cur.fetchall()
            for date, value in anomalies:
                if value < 0:
                    quality_check["anomalies"].append(f"{column} = {value:,.0f} (negative) on {date}")
                else:
                    quality_check["anomalies"].append(f"{column} = {value:,.0f} (>{threshold:,}) on {date}")
        
        # 4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Quality Score
        total_expected_records = 7 * len(transport_columns)  # 7 ‡∏ß‡∏±‡∏ô √ó 7 ‡∏™‡∏≤‡∏¢
        total_missing = sum(len(dates) for dates in quality_check["missing_data"].values())
        quality_check["quality_score"] = max(0, (total_expected_records - total_missing) / total_expected_records)
        
        # 5. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Status
        if quality_check["quality_score"] < 0.8:
            quality_check["status"] = "FAIL"
        elif quality_check["anomalies"] or total_missing > 0:
            quality_check["status"] = "WARNING"
        
        print(f"üìä MOT Data Quality Report:")
        print(f"   Records Processed: {quality_check['records_processed']}")
        print(f"   Quality Score: {quality_check['quality_score']:.2%}")
        
        if quality_check['missing_data']:
            print(f"   ‚ö†Ô∏è Data Issues:")
            for line, issues in quality_check['missing_data'].items():
                print(f"      {line}: {issues}")
        else:
            print(f"   ‚úÖ No missing data detected")
            
        if quality_check['anomalies']:
            print(f"   üîç Anomalies Found:")
            for anomaly in quality_check['anomalies']:
                print(f"      {anomaly}")
        else:
            print(f"   ‚úÖ No anomalies detected")
            
        print(f"   üìà Overall Status: {quality_check['status']}")
        
    except Exception as e:
        quality_check["status"] = "FAIL"
        quality_check["error"] = str(e)
        print(f"‚ùå MOT Quality Check Failed: {e}")
    
    finally:
        conn.close()
    
    # ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ú‡πà‡∏≤‡∏ô XCom
    context['task_instance'].xcom_push(key='mot_quality_result', value=quality_check)
    
    # Return branch decision
    if quality_check["status"] == "FAIL":
        return "mot_quality_failed"
    else:
        return "collect_rain_weather_data"


def check_rain_data_quality(**context):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Rain ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Collection
    """
    import json
    
    # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å MOT Quality Check
    mot_result = context['task_instance'].xcom_pull(key='mot_quality_result')
    
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    quality_check = {
        "task": "rain_quality_check", 
        "timestamp": datetime.now().isoformat(),
        "records_processed": 0,
        "null_rain_remaining": 0,
        "anomalies": [],
        "quality_score": 0.0,
        "status": "PASS"
    }
    
    try:
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö records ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô NULL
        cur.execute("""
            SELECT COUNT(*) FROM features 
            WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
            AND rain_average IS NULL
        """)
        quality_check["null_rain_remaining"] = cur.fetchone()[0]
        
        # 2. ‡∏ô‡∏±‡∏ö records ‡∏ó‡∏µ‡πà‡∏°‡∏µ rain data
        cur.execute("""
            SELECT COUNT(*) FROM features 
            WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
            AND rain_average IS NOT NULL
        """)
        quality_check["records_processed"] = cur.fetchone()[0]
        
        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ù‡∏ô‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
        cur.execute("""
            SELECT feature_date, rain_average FROM features 
            WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
            AND (rain_average < 0 OR rain_average > 200)
        """)
        anomalies = cur.fetchall()
        for date, value in anomalies:
            quality_check["anomalies"].append(f"rain = {value:.1f}mm on {date}")
        
        # 4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Quality Score
        total_records = mot_result.get("records_processed", 0) if mot_result else 7
        if total_records > 0:
            quality_check["quality_score"] = quality_check["records_processed"] / total_records
        
        # 5. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Status
        if quality_check["null_rain_remaining"] > 0:
            quality_check["status"] = "WARNING"
        if quality_check["quality_score"] < 0.8:
            quality_check["status"] = "FAIL"
            
        print(f"üåßÔ∏è Rain Data Quality Report:")
        print(f"   Records with Rain Data: {quality_check['records_processed']}")
        print(f"   NULL Rain Remaining: {quality_check['null_rain_remaining']}")
        print(f"   Quality Score: {quality_check['quality_score']:.2%}")
        print(f"   Anomalies: {quality_check['anomalies']}")
        print(f"   Status: {quality_check['status']}")
        
    except Exception as e:
        quality_check["status"] = "FAIL"
        quality_check["error"] = str(e)
        print(f"‚ùå Rain Quality Check Failed: {e}")
    
    finally:
        conn.close()
    
    # ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ú‡πà‡∏≤‡∏ô XCom
    context['task_instance'].xcom_push(key='rain_quality_result', value=quality_check)
    
    # Return branch decision
    if quality_check["status"] == "FAIL":
        return "rain_quality_failed"
    else:
        return "collect_day_type_data"


def check_day_type_data_quality(**context):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Day Type ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Collection
    """
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    
    quality_check = {
        "task": "day_type_quality_check",
        "timestamp": datetime.now().isoformat(),
        "records_processed": 0,
        "pending_day_type_remaining": 0,
        "classification_errors": [],
        "quality_score": 0.0,
        "status": "PASS"
    }
    
    try:
        # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö day_type ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô -1
        cur.execute("""
            SELECT COUNT(*) FROM features 
            WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
            AND day_type = -1
        """)
        quality_check["pending_day_type_remaining"] = cur.fetchone()[0]
        
        # 2. ‡∏ô‡∏±‡∏ö records ‡∏ó‡∏µ‡πà‡∏°‡∏µ day_type ‡πÅ‡∏•‡πâ‡∏ß
        cur.execute("""
            SELECT COUNT(*) FROM features 
            WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
            AND day_type != -1
        """)
        quality_check["records_processed"] = cur.fetchone()[0]
        
        # 3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Logic Validation (‡∏ß‡∏±‡∏ô‡πÄ‡∏™‡∏≤‡∏£‡πå-‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô day_type = 1)
        cur.execute("""
            SELECT feature_date, day_type, dow FROM features 
            WHERE feature_date >= CURRENT_DATE - INTERVAL '7 days'
            AND dow IN (6, 0)  -- ‡πÄ‡∏™‡∏≤‡∏£‡πå, ‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå
            AND day_type NOT IN (1, 2)  -- ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏±‡∏ô‡∏´‡∏¢‡∏∏‡∏î
        """)
        weekend_errors = cur.fetchall()
        for date, day_type, dow in weekend_errors:
            day_name = "Saturday" if dow == 6 else "Sunday"
            quality_check["classification_errors"].append(f"{day_name} {date} classified as day_type={day_type}")
        
        # 4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Quality Score
        mot_result = context['task_instance'].xcom_pull(key='mot_quality_result')
        total_records = mot_result.get("records_processed", 0) if mot_result else 7
        if total_records > 0:
            quality_check["quality_score"] = quality_check["records_processed"] / total_records
        
        # 5. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Status
        if quality_check["pending_day_type_remaining"] > 0:
            quality_check["status"] = "WARNING"
        if quality_check["classification_errors"]:
            quality_check["status"] = "WARNING"
        if quality_check["quality_score"] < 0.8:
            quality_check["status"] = "FAIL"
            
        print(f"üìÖ Day Type Data Quality Report:")
        print(f"   Records with Day Type: {quality_check['records_processed']}")
        print(f"   Pending Day Types: {quality_check['pending_day_type_remaining']}")
        print(f"   Classification Errors: {quality_check['classification_errors']}")
        print(f"   Quality Score: {quality_check['quality_score']:.2%}")
        print(f"   Status: {quality_check['status']}")
        
    except Exception as e:
        quality_check["status"] = "FAIL"
        quality_check["error"] = str(e)
        print(f"‚ùå Day Type Quality Check Failed: {e}")
    
    finally:
        conn.close()
    
    # ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ú‡πà‡∏≤‡∏ô XCom
    context['task_instance'].xcom_push(key='day_type_quality_result', value=quality_check)
    
    # Return branch decision
    if quality_check["status"] == "FAIL":
        return "day_type_quality_failed"
    else:
        return "final_quality_validation"


def generate_final_report(**context):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Quality Check
    mot_result = context['task_instance'].xcom_pull(key='mot_quality_result')
    rain_result = context['task_instance'].xcom_pull(key='rain_quality_result') 
    day_type_result = context['task_instance'].xcom_pull(key='day_type_quality_result')
    
    final_report = {
        "pipeline": "MOT-Rain-DayType Data Collection",
        "timestamp": datetime.now().isoformat(),
        "overall_status": "PASS",
        "stages": {
            "mot": mot_result,
            "rain": rain_result, 
            "day_type": day_type_result
        }
    }
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Overall Status
    statuses = [
        mot_result.get("status", "UNKNOWN") if mot_result else "UNKNOWN",
        rain_result.get("status", "UNKNOWN") if rain_result else "UNKNOWN", 
        day_type_result.get("status", "UNKNOWN") if day_type_result else "UNKNOWN"
    ]
    
    if "FAIL" in statuses:
        final_report["overall_status"] = "FAIL"
    elif "WARNING" in statuses:
        final_report["overall_status"] = "WARNING"
    
    print("=" * 60)
    print("üéØ FINAL PIPELINE QUALITY REPORT")
    print("=" * 60)
    print(f"Overall Status: {final_report['overall_status']}")
    print(f"Timestamp: {final_report['timestamp']}")
    print()
    
    for stage_name, result in final_report["stages"].items():
        if result:
            print(f"{stage_name.upper()} Stage:")
            print(f"  Status: {result.get('status', 'UNKNOWN')}")
            print(f"  Records: {result.get('records_processed', 0)}")
            print(f"  Quality Score: {result.get('quality_score', 0):.2%}")
            if result.get('anomalies'):
                print(f"  Anomalies: {len(result['anomalies'])}")
            print()
    
    # ‡∏™‡πà‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ú‡πà‡∏≤‡∏ô XCom
    context['task_instance'].xcom_push(key='final_report', value=final_report)

###############################################
# DAG Tasks
###############################################

# Start task
start_task = DummyOperator(
    task_id="start_weekly_data_collection",
    dag=dag
)


# MOT data collection task - ‡πÉ‡∏à‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡∏≠‡∏á workflow (‡πÉ‡∏ä‡πâ functional version)
mot_data_collection = SparkSubmitOperator(
    task_id="collect_mot_transport_data",
    application="/usr/local/spark/applications/mot_collecting.py",  # Functional version
    name="mot-data-collector-functional",
    conn_id=spark_conn,
    verbose=1,
    conf={
        # Simplified Spark configuration for Python script
        "spark.master": spark_master,
        "spark.executor.memory": "1g",
        "spark.driver.memory": "1g",
        "spark.driver.extraClassPath": postgres_driver_jar,
        "spark.executor.extraClassPath": postgres_driver_jar
    },
    application_args=[postgres_db, postgres_user, postgres_pwd],  # ‡∏™‡πà‡∏á args ‡πÑ‡∏õ‡∏¢‡∏±‡∏á script
    jars=postgres_driver_jar,
    driver_class_path=postgres_driver_jar,
    dag=dag,
)

# MOT Quality Check with Branching Decision
mot_quality_check = BranchPythonOperator(
    task_id="mot_quality_check",
    python_callable=check_mot_data_quality,
    dag=dag,
)

# Rain data collection task - ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ MOT quality ‡∏ú‡πà‡∏≤‡∏ô
rain_data_collection = SparkSubmitOperator(
    task_id="collect_rain_weather_data",
    application="/usr/local/spark/applications/rain_collecting.py",  # Rain functional version
    name="rain-data-collector-functional",
    conn_id=spark_conn,
    verbose=1,
    conf={
        # Simplified Spark configuration for Python script
        "spark.master": spark_master,
        "spark.executor.memory": "1g",
        "spark.driver.memory": "1g",
        "spark.driver.extraClassPath": postgres_driver_jar,
        "spark.executor.extraClassPath": postgres_driver_jar
    },
    application_args=[postgres_db, postgres_user, postgres_pwd],  # ‡∏™‡πà‡∏á args ‡πÑ‡∏õ‡∏¢‡∏±‡∏á script
    jars=postgres_driver_jar,
    driver_class_path=postgres_driver_jar,
    trigger_rule="none_failed_min_one_success",  # ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ upstream ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å SKIP
    dag=dag,
)

# Rain Quality Check with Branching Decision  
rain_quality_check = BranchPythonOperator(
    task_id="rain_quality_check",
    python_callable=check_rain_data_quality,
    dag=dag,
)

# Day type data collection task - ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ Rain quality ‡∏ú‡πà‡∏≤‡∏ô
day_type_data_collection = SparkSubmitOperator(
    task_id="collect_day_type_data",
    application="/usr/local/spark/applications/day_type_collecting.py",  # Day type functional version
    name="day-type-data-collector-functional",
    conn_id=spark_conn,
    verbose=1,
    conf={
        # Simplified Spark configuration for Python script
        "spark.master": spark_master,
        "spark.executor.memory": "1g",
        "spark.driver.memory": "1g",
        "spark.driver.extraClassPath": postgres_driver_jar,
        "spark.executor.extraClassPath": postgres_driver_jar
    },
    application_args=[postgres_db, postgres_user, postgres_pwd],  # ‡∏™‡πà‡∏á args ‡πÑ‡∏õ‡∏¢‡∏±‡∏á script
    jars=postgres_driver_jar,
    driver_class_path=postgres_driver_jar,
    trigger_rule="none_failed_min_one_success",  # ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ upstream ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å SKIP
    dag=dag,
)

# Day Type Quality Check with Branching Decision
day_type_quality_check = BranchPythonOperator(
    task_id="day_type_quality_check",
    python_callable=check_day_type_data_quality,
    dag=dag,
)

# Final Quality Validation - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ
final_quality_validation = PythonOperator(
    task_id="final_quality_validation",
    python_callable=generate_final_report,
    trigger_rule="none_failed_min_one_success",  # ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ upstream ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å SKIP
    dag=dag,
)

# Success/Failure endpoints
pipeline_success = DummyOperator(
    task_id="pipeline_success",
    trigger_rule="none_failed_min_one_success",  # ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ upstream ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å SKIP
    dag=dag
)

# Quality Check Failed Tasks
mot_quality_failed = DummyOperator(
    task_id="mot_quality_failed",
    dag=dag
)

rain_quality_failed = DummyOperator(
    task_id="rain_quality_failed", 
    dag=dag
)

day_type_quality_failed = DummyOperator(
    task_id="day_type_quality_failed",
    dag=dag
)

# End task
end_task = DummyOperator(
    task_id="end_weekly_data_collection",
    trigger_rule="none_failed_min_one_success",  # ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡∏à‡∏∞‡∏°‡∏µ paths ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å SKIP
    dag=dag
)

###############################################
# Task Dependencies & Workflow
###############################################

# Define the comprehensive data collection workflow with Quality Checks & Branching
# 
# Flow: Start ‚Üí MOT Collection ‚Üí MOT Quality Check ‚Üí Rain Collection ‚Üí Rain Quality Check 
#       ‚Üí Day Type Collection ‚Üí Day Type Quality Check ‚Üí Final Validation ‚Üí Success ‚Üí End
#       
# Quality Check Branching: Each stage can branch to PASS (continue) or FAIL (end pipeline)
# 
# 1. Start: Initialize the workflow
# 2. MOT Collection: Fetch new transportation data from MOT API (sets rain_average = NULL, day_type = -1)  
# 3. MOT Quality Check: Validate MOT data completeness, detect anomalies (BranchPythonOperator)
# 4. Rain Collection: Fill rain_average for records where it's NULL (if MOT quality passes)
# 5. Rain Quality Check: Validate rain data coverage and values (BranchPythonOperator)
# 6. Day Type Collection: Fill day_type for records where it's -1 (if Rain quality passes)
# 7. Day Type Quality Check: Validate day type classifications (BranchPythonOperator)  
# 8. Final Validation: Generate comprehensive quality report (PythonOperator with XCom)
# 9. Success/End: Complete the workflow with fully validated features table
# 
# XCom Usage: Quality metrics and results are shared between tasks for comprehensive reporting

# Complex Task Dependencies with Quality Checks & Branching
# 
# BranchPythonOperator Rules:
# - Each BranchPythonOperator must have direct downstream connections to ALL possible return task_ids
# - The returned task_id will be the ONLY task that runs next
# - Non-selected tasks will be marked as SKIPPED
#
# Trigger Rules Used:
# - Default (all_success): All upstream tasks must succeed
# - none_failed_min_one_success: At least one upstream succeeded, none failed (allows SKIPPED)
#
# Flow Diagram:
#
# start_task ‚Üí mot_data_collection ‚Üí mot_quality_check ‚Üí [collect_rain_weather_data | mot_quality_failed]
#                                                       ‚Üì                           ‚Üì
#                                            rain_quality_check ‚Üí [collect_day_type_data | rain_quality_failed]
#                                                                 ‚Üì                        ‚Üì
#                                                     day_type_quality_check ‚Üí [final_quality_validation | day_type_quality_failed]
#                                                                              ‚Üì                           ‚Üì
#                                                                    pipeline_success ‚Üí end_task ‚Üê ALL FAILED PATHS

# Main pipeline flow:
start_task >> mot_data_collection >> mot_quality_check

# MOT Quality Check branches (BranchPythonOperator returns either task_id)
mot_quality_check >> rain_data_collection
mot_quality_check >> mot_quality_failed

# Rain pipeline continuation
rain_data_collection >> rain_quality_check

# Rain Quality Check branches (BranchPythonOperator returns either task_id)  
rain_quality_check >> day_type_data_collection
rain_quality_check >> rain_quality_failed

# Day Type pipeline continuation
day_type_data_collection >> day_type_quality_check

# Day Type Quality Check branches (BranchPythonOperator returns either task_id)
day_type_quality_check >> final_quality_validation
day_type_quality_check >> day_type_quality_failed

# Final success path
final_quality_validation >> pipeline_success >> end_task

# All failed paths lead to end_task
mot_quality_failed >> end_task
rain_quality_failed >> end_task  
day_type_quality_failed >> end_task

###############################################
# DAG Documentation
###############################################

# ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö:
# - ‡πÇ‡∏Ñ‡πâ‡∏î‡∏´‡∏•‡∏±‡∏Å: 
#   * MOT: /usr/local/spark/applications/mot_collecting.py (‡πÉ‡∏ä‡πâ logic ‡∏à‡∏≤‡∏Å functional script)
#   * Rain: /usr/local/spark/applications/rain_collecting.py (‡πÉ‡∏ä‡πâ logic ‡∏à‡∏≤‡∏Å functional script)
#   * Day Type: /usr/local/spark/applications/day_type_collecting.py (‡πÉ‡∏ä‡πâ logic ‡∏à‡∏≤‡∏Å functional script)
# - ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: PostgreSQL features table (host: postgres, db: airflow)
# - API: MOT Open Data, OpenMeteo Weather, MyHora Holiday Calendar
# - APIs: 
#   * MOT Data: https://datagov.mot.go.th/api/action/datastore_search (MOT Open Data)
#   * Weather Data: https://api.open-meteo.com/v1/forecast (OpenMeteo API)
# - ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£: ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå‡πÄ‡∏ß‡∏•‡∏≤ 02:00 ‡∏ô. (Cron: "0 2 * * 0")
# - ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏™‡πà‡∏á + ‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
# - Features: 
#   * ‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 1 (MOT): ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏Ç‡∏ô‡∏™‡πà‡∏á‡πÉ‡∏´‡∏°‡πà + ‡∏ï‡∏±‡πâ‡∏á rain_average = NULL
#   * ‡∏Ç‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà 2 (Rain): ‡πÄ‡∏ï‡∏¥‡∏° rain_average ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô NULL
#   * ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
#   * ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å APIs
#   * ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á: MOT ‚Üí Rain ‚Üí Validation
#   * Upsert ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö intelligent
